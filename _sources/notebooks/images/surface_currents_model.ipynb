{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6812586e-cccb-4281-b6c3-e58752f319ef",
   "metadata": {},
   "source": [
    "## Model and Related Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8d25c-2263-4d67-a85a-e660858eb78d",
   "metadata": {},
   "source": [
    "First up is a callback for tracking loss history. This is helpful because we can use it to view how training is progressing. The specifics aren't too important, but it can be altered to trigger at different frequencies and on different events (like an epoch end vs. a batch end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c02cd823-33ca-4168-8890-7124830a1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, frequency=2048):\n",
    "        self.frequency = frequency\n",
    "        self.batch_counter = 0\n",
    "        \n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.mae = []\n",
    "        self.mse = []\n",
    "        self.accuracy = []\n",
    "        self.logs = []\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # if batch % self.frequency == 0:\n",
    "        self.batch_counter += self.frequency\n",
    "        self.x.append(self.i)\n",
    "        self.mae.append(logs.get('mae'))\n",
    "        self.mse.append(logs.get('mse'))\n",
    "        self.accuracy.append(logs.get('accuracy'))\n",
    "\n",
    "        self.i += 1\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"MAE: {self.mae[-1]} \\t\\t MSE: {self.mse[-1]} \\t\\t Accuracy: {self.accuracy[-1]}\")\n",
    "        \n",
    "        plt.figure(figsize=(18,5))\n",
    "\n",
    "        plt.subplot(131)\n",
    "        plt.plot(self.x, self.mae, color='#ff6347',label=\"mae\")\n",
    "        plt.plot(self.x[-1], self.mae[-1],marker = 'o', markersize=10, color='#ff6347')\n",
    "        plt.legend()\n",
    "        plt.xlabel(r'batch');\n",
    "        plt.ylabel('Mean Absolute Error');\n",
    "        plt.ylim([0.,100.])\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.plot(self.x, self.mse, color='#6495ed')\n",
    "        plt.plot(self.x[-1], self.mse[-1],marker = 'o', markersize=10, color='#6495ed')\n",
    "        plt.xlabel('batch')\n",
    "        plt.ylabel(r'Mean Squared Error [$cm^2/s^2$]')\n",
    "        plt.ylim([0.,1000.])\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.plot(self.x, self.accuracy, color='#3cb371')\n",
    "        plt.plot(self.x[-1], self.accuracy[-1],marker = 'o', markersize=10, color='#3cb371')\n",
    "        plt.xlabel('batch')\n",
    "        plt.ylabel('Model Accuracy')\n",
    "        plt.ylim([0.,1.])\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43dd9cc-992c-46c3-852c-d8135a3bbeb1",
   "metadata": {},
   "source": [
    "We define a custom loss function to take the MAE of multidimensional objects. (This should be able to filter NaNs, thereby solving the coastline problem, but this doesn't work!). Not sure if this gives different answers from a standard MAE in the case that no NaNs are present. When NaNs are present, this should give different results from using `.fillna(0)` on the training data, because it won't entrain zeros when the model takes a convolution. But does it actually work? ¯\\\\\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0482c1db-9481-46c0-b4e6-164ff7f0adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corner case: what happens when everything is NaN?\n",
    "class Grid_MAE(tf.keras.losses.Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        avg = tf.math.abs(y_true - y_pred)\n",
    "        masked = tf.where(tf.math.is_finite(avg), avg, tf.zeros_like(avg))\n",
    "        return tf.math.reduce_sum(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9656541-5615-4914-886a-95cc1bf3d7cc",
   "metadata": {},
   "source": [
    "The `get_model` function generates a neural network based on Sinha and Abernathey (2021), but offers some parameters to enable a broader class of neural networks of similar form.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f27dbab-4e71-4228-8ec4-43d1226b22da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(halo_size, ds, sc, conv_dims, nfilters, conv_kernels, dense_layers):\n",
    "\n",
    "    conv_init = tf.keras.Input(shape=tuple(conv_dims) + (len(sc.conv_var),))\n",
    "    last_layer = conv_init\n",
    "    for kernel in conv_kernels:\n",
    "        this_layer = tf.keras.layers.Conv2D(nfilters, kernel)(last_layer)\n",
    "        last_layer = this_layer\n",
    "        nfilters = nfilters / 2.\n",
    "    \n",
    "    halo_dims = [x - 2*halo_size for x in conv_dims]\n",
    "    input_init = tf.keras.Input(shape=tuple(halo_dims) + (len(sc.input_var),))\n",
    "    last_layer = tf.keras.layers.concatenate([last_layer, input_init])\n",
    "    last_layer = tf.keras.layers.LeakyReLU(alpha=0.3)(last_layer)\n",
    "    for layer in range(dense_layers):\n",
    "        this_layer = tf.keras.layers.Dense(nfilters, activation='relu')(last_layer)\n",
    "        last_layer = this_layer\n",
    "        nfilters = nfilters / 2.\n",
    "        \n",
    "    output_layer = tf.keras.layers.Dense(len(sc.target))(last_layer)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[conv_init, input_init], outputs=output_layer)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(loss=Grid_MAE(), optimizer=opt, metrics=['mae', 'mse', 'accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4538503-3fb4-4fa9-b677-2167cadcb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ds, sc, conv_dims=[3,3], nfilters=80, conv_kernels=[3], dense_layers=3):\n",
    "    pars = locals()\n",
    "\n",
    "    halo_size = int((np.sum(conv_kernels) - len(conv_kernels))/2)\n",
    "    halo_dims = [x - 2*halo_size for x in conv_dims]\n",
    "    \n",
    "    nlons, nlats = conv_dims\n",
    "\n",
    "    bgen = xb.BatchGenerator(\n",
    "        ds,\n",
    "        {'nlon':nlons,       'nlat':nlats},\n",
    "        {'nlon':2*halo_size, 'nlat':2*halo_size},\n",
    "        concat_input_dims=True\n",
    "    )\n",
    "    \n",
    "    # We need this subsetting stencil to compensate for the fact that a halo is\n",
    "    # removed by each convolution layer. This means that the input_var variables\n",
    "    # will be the wrong size at the concat layer unless we strip a halo from them\n",
    "    sub = {'nlon_input':range(halo_size,nlons-halo_size),\n",
    "           'nlat_input':range(halo_size,nlats-halo_size)}\n",
    "\n",
    "    model = get_model(halo_size, halo_dims, **pars)\n",
    "    history = LossHistory()\n",
    "    \n",
    "    # def batcher(bgen, batch_size=32):\n",
    "    #     i = 0\n",
    "    #     while True:\n",
    "    #         # print(\"{},\\t\\t{}\".format(batch_size*i, batch_size*i + batch_size))\n",
    "    #         yield xr.concat(itertools.islice(bgen, batch_size*i, batch_size*i + batch_size), dim='sample').transpose('sample', ...)\n",
    "    #         i += 1\n",
    "        \n",
    "    # print(batcher(bgen))\n",
    "    \n",
    "    # b = batcher(bgen)\n",
    "    \n",
    "    for batch in bgen:\n",
    "        \n",
    "        batch_conv   = [batch[x] for x in sc.conv_var]\n",
    "        batch_input  = [batch[x][sub] for x in sc.input_var]\n",
    "        batch_target = [batch[x][sub] for x in sc.target]\n",
    "        batch_conv   = xr.merge(batch_conv).to_array('var').transpose(...,'var')\n",
    "        batch_input  = xr.merge(batch_input).to_array('var').transpose(...,'var')\n",
    "        batch_target = xr.merge(batch_target).to_array('var').transpose(...,'var')\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        model.fit([batch_conv, batch_input],\n",
    "                  batch_target,\n",
    "                  batch_size=32, epochs=4, verbose=0,\n",
    "                  callbacks=[history])\n",
    "\n",
    "    model.save('models/'+ sc.name)\n",
    "    np.savez('models/history_'+sc.name, losses=history.mae, mse=history.mse, accuracy=history.accuracy)\n",
    "\n",
    "    return model, history\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
